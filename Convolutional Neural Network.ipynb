{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7359cfd",
   "metadata": {},
   "source": [
    "- since it is not a industrial application, we have differnet steps for pre-processing, implementation etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b8410",
   "metadata": {},
   "source": [
    "# THEORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46308d7f",
   "metadata": {},
   "source": [
    "# 1. Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b5be1",
   "metadata": {},
   "source": [
    "- uses an <b> input image </b>, a <b> feature detector </b> to map the input image to a <b> feature map </b>\n",
    "- say input image is 7x7, feature detector basically  <i><b> convolutes </b></i> a 3x3 from the input image with another 3x3 grid (separately chosen) and assigns a number to it on the feature map (here it is 5x5)\n",
    "- convolution amongst matrices can be simply thought of as multiplying corresponding elements\n",
    "- another name for the feature map is convolved feature/activation map <br><br>\n",
    "- first main objective is to reduce the size to process it faster, with some loss of info; but we are mainly detecting features and patterns, and not pixel by pixel, so it doesn't really matter<br>\n",
    "- many differnet convolutions are obtained, that is many different feature maps are obtained by changing the feature detector; this constitutes a <b> convolution </b> type of layer <br><br>\n",
    "- a specific example of feature detector could be the sharpen; it is 5x5 matrix with value 5 in the middle, surrounded by -1, and then 0s, ultimately giving more weightage to the middle pixel in every convolution <br><br>\n",
    "- next we apply ReLU, it helps to break linearity, and speed up training, it's a standard step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fab553",
   "metadata": {},
   "source": [
    "# 2. Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36022f6",
   "metadata": {},
   "source": [
    "- next step after Convolution and ReLU activation function\n",
    "- helps in image recognition where we need to detect object despite their spatial variety, i.e, even the if the images are tilted, compressed or blurred out, the NN should be able to detect the object as is in training set\n",
    "- we create a pooled feature map from the orig feature map:\n",
    "    - in <b> max pooling </b>, the pooled feature map contains only max values from a certain sized grid, say we considered a 5x5 feature map, now we want a pooled feature map viz 3x3, then we consider 2x2 grids from the feature map, and choose only the max values (the size of skips in between grids can be varied)\n",
    "    - we still preserve the feature, because when we choose the max value corresponding to a particular feature, we retain the weights associated\n",
    "    - in a way, it also helps to prevent overfitting\n",
    "    - all the parameters regarding grid size, selecting max value, stride etc can be tuned and tested\n",
    "- other methods of pooling are sub-sampling/ <b> mean pooling</b>, <b> sum pooling </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eddfb9",
   "metadata": {},
   "source": [
    "# 3. Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28479855",
   "metadata": {},
   "source": [
    "- simple step after convolution and max pooling\n",
    "- row by row transferring such that final pooled feature map becomes a 1D vector\n",
    "- input layer for Artificial NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71157b4e",
   "metadata": {},
   "source": [
    "# 4. Full Connection using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed348af",
   "metadata": {},
   "source": [
    "- flattened layer is the input layer\n",
    "- all hidden layers are completely connected format\n",
    "- basically now it is similar to ANN, all hidden layers choose more set of features and create newer features, use of back prop to propogate the error backwards, we <b>also update feature detectors</b>\n",
    "- so propogation goes from the very first convolution layer to output layer and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e58f8",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17c724",
   "metadata": {},
   "source": [
    "- generalisation of logistic regression, when input classes are more than 2, the output probabilities are obtained using the softmax formula\n",
    "- we use this along with minimising <b>Categorical CrossEntropy</b> loss function for the classification task in the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054f9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5235d648",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f0ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fba64",
   "metadata": {},
   "source": [
    "- helps data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a592bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753eb216",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "- to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf22d13",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing on Train Data\n",
    "- image augmentation process: shifting of pixels, changing intensitoes, zoom in/out, rotataion of pixels\n",
    "- specifically \"shear range\", \"zoom range\", \"horizontal flip\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0feb9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this object will apply all the transformations on the image data sets\n",
    "train_datagen= ImageDataGenerator(\n",
    "            rescale=1./255,       # feature scaling, i.e, it divides all pixel values by 255\n",
    "            shear_range=0.2,      # literally, applying a shear strain to the image\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793d78c",
   "metadata": {},
   "source": [
    "- we need to split image datasets into images before passing it into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89b1143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data= train_datagen.flow_from_directory(\n",
    "            'dataset/training_set', # how it is stored in the directory\n",
    "            target_size=(64,64),    # size of image being sent into NN\n",
    "            batch_size=32,          # number of images ina batch=32\n",
    "            class_mode='binary')    # output layer's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b491cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_data= test_datagen.flow_from_directory(\n",
    "            'dataset/test_set',     # how it is stored in the directory\n",
    "            target_size=(64,64),    # size of image being sent into NN\n",
    "            batch_size=32,          # number of images ina batch=32\n",
    "            class_mode='binary')    # output layer's prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02f294",
   "metadata": {},
   "source": [
    "## 2. Builidng the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7407d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593051f",
   "metadata": {},
   "source": [
    "### 2.1 Convolution\n",
    "- parameters:-\n",
    "    - <mark>filters</mark>: number of feature detectors\n",
    "               aka kernel\n",
    "    - <mark>kernel_size</mark>: dimensions of feature detector (n x n)\n",
    "    - <mark>activation</mark> : ReLU (for all non output layers)\n",
    "    - 4th parameter comes under <b> **kwargs </b> so no need to mention parameter name, this is actually the shape of the input image, viz r x c x n\n",
    "        - r is number of pixels in a row, c is number of pixels in a column, n is number of base colors, here = 3, as we use RGB scheme\n",
    "    - others include <mark>stride</mark> (r x c), <mark>padding</mark> etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657d9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size= 3, activation= 'relu', input_shape=[64,64,3])) \n",
    "# Conv2D helps to build the convolution layer (diff from dense),\n",
    "# we are using the classical architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793432ef",
   "metadata": {},
   "source": [
    "### 2.2 Pooling\n",
    "- parameters:-\n",
    "    - <mark>pool_size</mark> : the dimensions of small square frame which is used to read off the feature map and obtain the new values, in case of max_pooling, we get the maximum amongst these values in the frame\n",
    "    - <mark>stride</mark>: number of shifts from one frame to next frame\n",
    "    - <mark>padding</mark>: set as 'valid' when we need to ignore extra space in the frame which doesn't cover the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21830c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides= 2, padding= 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d2667",
   "metadata": {},
   "source": [
    "- 2nd convolution and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22335f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size= 3, activation= 'relu')) # no need of input shape, keep the no.of filter same\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides= 2, padding= 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f7157",
   "metadata": {},
   "source": [
    "### 2.3 Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ba5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7eaaac",
   "metadata": {},
   "source": [
    "### 2.4 Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca26d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation= 'relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4368191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd9e1521",
   "metadata": {},
   "source": [
    "## 3. Training the CNN\n",
    "- compilation followed by training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d1759",
   "metadata": {},
   "source": [
    "### 3.1 Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f3c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # same as ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cea550",
   "metadata": {},
   "source": [
    "### 3.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dd71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 90s 351ms/step - loss: 0.6685 - accuracy: 0.5879 - val_loss: 0.7285 - val_accuracy: 0.5616\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.5924 - accuracy: 0.6814 - val_loss: 0.5353 - val_accuracy: 0.7387\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.5568 - accuracy: 0.7155 - val_loss: 0.5303 - val_accuracy: 0.7367\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 78s 313ms/step - loss: 0.5250 - accuracy: 0.7356 - val_loss: 0.5148 - val_accuracy: 0.7437\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.5001 - accuracy: 0.7445 - val_loss: 0.4829 - val_accuracy: 0.7668\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4921 - accuracy: 0.7571 - val_loss: 0.5030 - val_accuracy: 0.7573\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4699 - accuracy: 0.7778 - val_loss: 0.4824 - val_accuracy: 0.7718\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4498 - accuracy: 0.7883 - val_loss: 0.4908 - val_accuracy: 0.7718\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.4388 - accuracy: 0.7985 - val_loss: 0.5029 - val_accuracy: 0.7653\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.4202 - accuracy: 0.8036 - val_loss: 0.4759 - val_accuracy: 0.7818\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4067 - accuracy: 0.8138 - val_loss: 0.4465 - val_accuracy: 0.7898\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4048 - accuracy: 0.8166 - val_loss: 0.4495 - val_accuracy: 0.7913\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.3881 - accuracy: 0.8226 - val_loss: 0.4369 - val_accuracy: 0.8008\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 211s 845ms/step - loss: 0.3631 - accuracy: 0.8403 - val_loss: 0.4553 - val_accuracy: 0.7958\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.3523 - accuracy: 0.8421 - val_loss: 0.4665 - val_accuracy: 0.7993\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.3402 - accuracy: 0.8526 - val_loss: 0.4940 - val_accuracy: 0.7908\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.3273 - accuracy: 0.8569 - val_loss: 0.4447 - val_accuracy: 0.8128\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.3245 - accuracy: 0.8583 - val_loss: 0.4722 - val_accuracy: 0.7908\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 0.3014 - accuracy: 0.8692 - val_loss: 0.4666 - val_accuracy: 0.8143\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.2893 - accuracy: 0.8759 - val_loss: 0.4823 - val_accuracy: 0.8038\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 77s 309ms/step - loss: 0.2689 - accuracy: 0.8873 - val_loss: 0.5333 - val_accuracy: 0.7908\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.2638 - accuracy: 0.8882 - val_loss: 0.4856 - val_accuracy: 0.8018\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.2469 - accuracy: 0.8970 - val_loss: 0.5090 - val_accuracy: 0.8108\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 78s 310ms/step - loss: 0.2346 - accuracy: 0.9029 - val_loss: 0.5063 - val_accuracy: 0.8063\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 77s 309ms/step - loss: 0.2230 - accuracy: 0.9114 - val_loss: 0.5561 - val_accuracy: 0.7968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a087cac810>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_data, validation_data=test_data, epochs= 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec3d7f",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b77f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f985b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the path of the image in a variable and resize the image acc to input to convolution layer\n",
    "test_image = image.load_img(\"dataset/single_prediction/cat_or_dog_4.jpg\", target_size=(64,64))\n",
    "# this gives in PIL format, convert to array format\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "# the CNN is modelling the data in certain number of batches, each of size 32, therefore even the image to be tested on needs\n",
    "# to be in the form of a batch, this ensures that the predict method sees the extra dimension corresponding to the batch\n",
    "# axis of the new expanded array to which we need to add the dimension is the 1st dimension, therefore axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daccbe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eafe358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f61150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 227ms/step\n"
     ]
    }
   ],
   "source": [
    "result = cnn.predict(test_image/255.)\n",
    "final_prediction = 'dog' if result[0][0] > 0.5 else 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78281a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28e053dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec7038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
